I"H”<p><br /></p>

<p><a href="https://statistics.laerd.com/spss-tutorials/multiple-regression-using-spss-statistics.php">
Laerd
</a></p>

<h4>Make_Regression</h4>

<p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html">
Scikit-learn
</a></p>

<p><a href="https://nbviewer.jupyter.org/github/cliffwhitworth/machine_learning_notebooks/blob/master/SimpleLinearRegression.ipynb">
Notebook
</a></p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Create X y dataset</span>
<span class="no">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">coef</span> <span class="o">=</span> <span class="no">True</span><span class="p">)</span></code></pre></figure>

<h4>Plot and visualize the data</h4>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Plot data</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="no">X</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()),</span> <span class="n">np</span><span class="p">.</span><span class="nf">poly1d</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">polyfit</span><span class="p">(</span><span class="no">X</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="no">X</span><span class="p">.</span><span class="nf">flatten</span><span class="p">())))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="no">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span></code></pre></figure>

<h4>Create Dataframe</h4>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Create a dataframe of the feature and add the target</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="no">DataFrame</span><span class="p">(</span><span class="no">X</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'X'</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Dataframe Head'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">())</span></code></pre></figure>

<h4>Descriptive Stats</h4>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Print descriptive stats</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">describe</span><span class="p">())</span></code></pre></figure>

<h4>Regression Stats</h4>

<p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html">
SciPy
</a></p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Calculate linear least-squares regression</span>
<span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">r_value</span><span class="p">,</span> <span class="n">p_value</span><span class="p">,</span> <span class="n">std_err</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="nf">linregress</span><span class="p">(</span><span class="no">X</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'slope: '</span><span class="p">,</span> <span class="n">slope</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'intercept: '</span><span class="p">,</span> <span class="n">intercept</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'r_value: '</span><span class="p">,</span> <span class="n">r_value</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'r_squared: '</span><span class="p">,</span> <span class="n">r_value</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'p_value: '</span><span class="p">,</span> <span class="n">p_value</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">'std_err: '</span><span class="p">,</span> <span class="n">std_err</span><span class="p">)</span></code></pre></figure>

<h4>OLS Model</h4>

<p><a href="http://www.statsmodels.org/dev/examples/notebooks/generated/formulas.html">
Statsmodels
</a></p>

<p><a href="https://www.learndatasci.com/tutorials/predicting-housing-prices-linear-regression-using-python-pandas-statsmodels/">
Towards Data Science
</a></p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ols</span><span class="p">(</span><span class="s2">"y ~ X"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">).</span><span class="nf">fit</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span></code></pre></figure>

<h4>Confidence Intervals</h4>

<p><a href="https://www.statsmodels.org/dev/examples/notebooks/generated/wls.html">
Statsmodels (WLS)
</a></p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Retrieve our confidence interval values with wls_prediction_std</span>

<span class="n">_</span><span class="p">,</span> <span class="n">confidence_interval_lower</span><span class="p">,</span> <span class="n">confidence_interval_upper</span> <span class="o">=</span> <span class="n">wls_prediction_std</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="c1"># Plot data</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">'o'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'data'</span><span class="p">)</span>

<span class="c1"># Plot trend line</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">fittedvalues</span><span class="p">,</span> <span class="s1">'g-.'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'OLS'</span><span class="p">)</span>

<span class="c1"># Plot confidence interval</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">confidence_interval_upper</span><span class="p">,</span> <span class="s1">'r-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Confidence Intervals'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">confidence_interval_lower</span><span class="p">,</span> <span class="s1">'r-'</span><span class="p">)</span>

<span class="c1"># Plot legend</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'best'</span><span class="p">);</span></code></pre></figure>

<h4>Regression Plots</h4>

<p><a href="https://www.statsmodels.org/dev/generated/statsmodels.graphics.regressionplots.plot_regress_exog.html">
Statsmodels
</a></p>

<p><a href="https://www.statsmodels.org/dev/endog_exog.html">
Endog vs Exog
</a></p>

<p><a href="https://www.learndatasci.com/tutorials/predicting-housing-prices-linear-regression-using-python-pandas-statsmodels/">
Towards Data Science (Regression Plots)
</a></p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Regression plots</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nf">graphics</span><span class="p">.</span><span class="nf">plot_regress_exog</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">"X"</span><span class="p">,</span> <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">)</span></code></pre></figure>

<h4>Probability Plot</h4>

<p><a href="https://pythonfordatascience.org/linear-regression-python/">
Probability Plot
</a></p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Probability Plot</span>
<span class="n">stats</span><span class="p">.</span><span class="nf">probplot</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">resid</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="s2">"norm"</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span> <span class="n">plt</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s2">"Residuals Q-Q Plot"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span></code></pre></figure>

<h4>Assumptions</h4>

<p><a href="https://www.youtube.com/watch?v=iMdtTCX2Q70">
Youtube
</a></p>

<h4>Assumption of Independent Errors</h4>

<p><a href="http://www.biostathandbook.com/independence.html">
Assumption of Independence
</a></p>

<p><a href="https://pythonfordatascience.org/linear-regression-python/">
Durbin Watson
</a></p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Assumption of independent errors</span>
<span class="nb">print</span><span class="p">(</span><span class="n">statsmodels</span><span class="p">.</span><span class="nf">stats</span><span class="p">.</span><span class="nf">stattools</span><span class="p">.</span><span class="nf">durbin_watson</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">resid</span><span class="p">))</span></code></pre></figure>

<h4>Assumption of Residual Normality</h4>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Assumption of normality of the residuals</span>
<span class="nb">name</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Jarque-Bera'</span><span class="p">,</span> <span class="s1">'Chi^2 two-tail prob.'</span><span class="p">,</span> <span class="s1">'Skew'</span><span class="p">,</span> <span class="s1">'Kurtosis'</span><span class="p">]</span>
<span class="nb">test</span> <span class="o">=</span> <span class="n">sms</span><span class="p">.</span><span class="nf">jarque_bera</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">resid</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lzip</span><span class="p">(</span><span class="nb">name</span><span class="p">,</span> <span class="nb">test</span><span class="p">))</span></code></pre></figure>

<h4>Assumption of Homoscedasticity</h4>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Assumption of homoscedasticity</span>
<span class="nb">name</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Lagrange multiplier statistic'</span><span class="p">,</span> <span class="s1">'p-value'</span><span class="p">,</span> <span class="s1">'f-value'</span><span class="p">,</span> <span class="s1">'f p-value'</span><span class="p">]</span>
<span class="nb">test</span> <span class="o">=</span> <span class="n">sms</span><span class="p">.</span><span class="nf">het_breuschpagan</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">resid</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">model</span><span class="p">.</span><span class="nf">exog</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lzip</span><span class="p">(</span><span class="nb">name</span><span class="p">,</span> <span class="nb">test</span><span class="p">))</span></code></pre></figure>

<h4>Gradient Descent Example</h4>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>

    <span class="k">return</span> <span class="p">((</span><span class="no">X</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="no">T</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">y</span><span class="p">.</span><span class="nf">size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gradientDescent</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">):</span>

    <span class="c1"># Initialize values</span>
    <span class="no">J_history</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">num_iters</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'r'</span><span class="p">,</span> <span class="s1">'g'</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="s1">'y'</span><span class="p">,</span> <span class="s1">'c'</span><span class="p">]</span>
    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Values for the line equation from the first several iterations of the gradient descent'</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>       
        <span class="c1"># beta = beta - alpha * (X.T.dot(X.dot(beta)-y)/m)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="no">X</span><span class="o">.</span><span class="no">T</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="no">X</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="no">T</span><span class="p">))</span>

        <span class="c1"># cost history    </span>
        <span class="no">J_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">30</span> <span class="n">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">6</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Show some thetas and costs in the line equation as it approaches best fit</span>
            <span class="c1"># Assuming convergence is before 30 iterations</span>
            <span class="nb">print</span> <span class="p">(</span><span class="s1">'Iteration {}: y = {:0.4f} + {:0.4f}x and the cost: {:0.4f}'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="no">J_history</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>
            <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="no">Xcopy</span><span class="p">,</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="no">Xcopy</span><span class="p">,</span> <span class="s1">'-'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
            <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="no">J_history</span>

<span class="c1"># Andrew Ng's M&amp;Ns</span>
<span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="no">X</span><span class="p">.</span><span class="nf">shape</span> <span class="c1"># observations, features</span>

<span class="c1"># Save original X</span>
<span class="no">Xcopy</span> <span class="o">=</span> <span class="no">X</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
<span class="no">Xcopy</span> <span class="o">=</span> <span class="no">Xcopy</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span>

<span class="c1"># Reshape X and add bias</span>
<span class="no">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">y</span><span class="p">.</span><span class="nf">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="n">int</span><span class="p">),</span> <span class="n">values</span> <span class="o">=</span> <span class="no">X</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="nf">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Plot equation lines based on gradient descent</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>

<span class="c1"># Plot data</span>
<span class="c1"># y = 0.1383 + 0.7234x</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="no">Xcopy</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># plt.plot(Xcopy, a1 + b1 * Xcopy, 'r-', linewidth=3)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="no">Xcopy</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">poly1d</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">polyfit</span><span class="p">(</span><span class="no">Xcopy</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="no">Xcopy</span><span class="p">)),</span> <span class="s1">'k-'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="no">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s1">'Lines converging on best fit'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="s1">'X'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="s1">'y'</span><span class="p">)</span>

<span class="c1"># Choose a learning rate</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">num_iters</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Init weights and run gradient descent</span>
<span class="c1"># theta = np.zeros((X.shape[1], 1))</span>
<span class="n">theta</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">theta</span><span class="p">,</span> <span class="no">J_history</span> <span class="o">=</span> <span class="n">gradientDescent</span><span class="p">(</span><span class="no">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">)</span>

<span class="c1"># Get slope and intercept</span>
<span class="c1"># denominator = y.size * sum(map(lambda x:x*x,X)) - X.sum()**2</span>
<span class="c1"># a = ((y.sum() * X.dot(X)) - (X.sum() * sum(X * y))) / denominator</span>
<span class="c1"># b = ((y.size * sum(X * y)) - (X.sum() * y.sum())) / denominator</span>
<span class="c1"># print()</span>
<span class="c1"># print ('y = {:0.4f} + {:0.4f}x'.format(a, b))</span>

<span class="c1"># Similar method to get slope and intercept</span>
<span class="n">d</span> <span class="o">=</span> <span class="no">Xcopy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="no">Xcopy</span><span class="p">)</span> <span class="o">-</span> <span class="no">Xcopy</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="no">Xcopy</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">a1</span> <span class="o">=</span> <span class="p">(</span> <span class="n">y</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="no">Xcopy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="no">Xcopy</span><span class="p">)</span> <span class="o">-</span> <span class="no">Xcopy</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="no">Xcopy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="p">)</span> <span class="o">/</span> <span class="n">d</span>
<span class="n">b1</span> <span class="o">=</span> <span class="p">(</span> <span class="no">Xcopy</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="no">Xcopy</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="p">)</span> <span class="o">/</span> <span class="n">d</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">'Via formula: y = {:0.4f} + {:0.4f}x'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">b1</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>

<span class="c1"># Plot the graph</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">range</span><span class="p">(</span><span class="no">J_history</span><span class="p">.</span><span class="nf">size</span><span class="p">),</span> <span class="no">J_history</span><span class="p">,</span> <span class="s2">"-b"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s1">'Convergence of J(\u03B8) Against Iteration'</span><span class="p">)</span>
<span class="c1"># r'J($\theta$)'</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="s1">'Number of iterations'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="s1">'Cost J'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="no">False</span><span class="p">)</span></code></pre></figure>

<h4>Split Train and Test</h4>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="c1"># Splitting the dataset into the Training set and Test set</span>
<span class="n">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="nf">model_selection</span> <span class="n">import</span> <span class="n">train_test_split</span>
<span class="no">X_train</span><span class="p">,</span> <span class="no">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="no">Xcopy</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="nf">size</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Fitting Simple Linear Regression to the Training set</span>
<span class="n">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="nf">linear_model</span> <span class="n">import</span> <span class="no">LinearRegression</span>
<span class="n">regressor</span> <span class="o">=</span> <span class="no">LinearRegression</span><span class="p">()</span>
<span class="n">regressor</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predicting the Test set results</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="no">X_test</span><span class="p">)</span>

<span class="c1"># The intercept coefficient</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Intercept: '</span><span class="p">,</span> <span class="n">regressor</span><span class="p">.</span><span class="nf">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Coefficients: '</span><span class="p">,</span> <span class="n">regressor</span><span class="p">.</span><span class="nf">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># The mean squared error mse</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Mean squared error: %.2f"</span> <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="c1"># The root mean squared error rmse</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Root mean squared error: %.2f"</span> <span class="o">%</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>
<span class="c1"># The mean absolute error mae</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Mean absolute error: %.2f"</span> <span class="o">%</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="c1"># Explained variance score: 1 is perfect prediction</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Variance score: %.2f'</span> <span class="o">%</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># Visualising the Training set results</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span> <span class="n">regressor</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="no">X_train</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'blue'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s1">'Training Set'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="s1">'X'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="s1">'y'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Visualising the Test set results</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="no">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="no">X_train</span><span class="p">,</span> <span class="n">regressor</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="no">X_train</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'blue'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s1">'Test Set'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="s1">'X'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="s1">'y'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span></code></pre></figure>

:ET